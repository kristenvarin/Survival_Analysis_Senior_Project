# Cox Proportional Hazards Regression

## Introduction

So far, we have dealt with calculating survival probability using a Kaplan Meier curve. While a Kaplan Meier curve is useful for predicting survival probability for simple data, it can not always be used since its a nonparametric method, meaning it follows no prior assumptions about the data. When data becomes more complex and variables about individuals need to be considered, other methods need to be used. One of these methods, proportional hazard analysis, similarly focuses on time-to-event data, but can be used for more realistic scenarios that deal with additional explanatory variables for the event of interest (@clark03). For example, we can use hazard analysis to calculate the probability of a person recovering from a disease based on type of treatment, while also including variables that could affect the outcome such as age, sex or history of drugs. A hazard function can be more useful during real-world analysis than the Kaplan Meier curve previously discussed.

We will again create a simple data set to demonstrate the process. Let's recall the `surv2` data set we created earlier, now with an additional variable, age @tbl-surv3.

```{r}
# Load Packages
library(tidyverse) 
library(knitr)
library(survival)
library(ggsurvfit)
library(gt)
```

```{r}
# Recreate surv2 and add age variable
time <- c(2, 7, 9, 8, 5, 3, 4, 10, 6, 1)
status <- c(0, 1, 0, 1, 0, 1, 0, 0, 1, 1)
group <- c(1, 1, 1, 1, 1, 2, 2 ,2, 2, 2)
age <- c(40, 62, 37, 67, 44, 70, 50, 45, 61, 62)
surv3 <- data.frame(time, status, group, age) 
```

```{r}
#| label: tbl-surv3
#| tbl-cap: "Example Data Set with Status, Time, Group, and Age"

surv3 %>% gt(caption = "Example Data Set with Status, Time, Group, and Age") %>%
  cols_label(time = "Time", status = "Status", group = "Group", age = "Age") 
```

## Proportional Hazards

The proportional hazards model, also known as the Cox regression model, is a semi-parametric model. This is because it assumes proportional hazards, or that the hazard of an event occurring is the same for all individuals, yet does not assume a specific distribution (@collet). The proportional hazard model also assumes that the hazard ratio between two groups is constant over time. This means that the hazard ratio between two groups is the same at any time $t$, thus making the model proportional. In other words, the instantaneous hazard of an event occurring between two individuals of different groups will remain constant at all times, or the effect of the predictors is the same at all times (@sthda).

## Hazard Ratios

The goal of hazard analysis is to create a hazard function for modelling time to event data, where the outcome $h(t)$ or $λ(t)$ is the probability of the event occurring for a subject who has lasted until time $t$ (@clark03). The hazard function is modeled using hazard ratios, which express the ratio between the hazard of an event between two groups at a time. The hazard ratio between two groups can be expressed as $v = \frac{h_1(t)}{h_2(t)}$ (@collet).

## Cox Proportional Hazards Model

The Cox proportional hazards model is expressed in terms of the hazard function on an individual $i$ at time $t$. This function is expressed as $h_i(t) = vh_0(t)$, where $h_0(t)$ is the baseline hazard function and $v$ is the hazard ratio (@collet). The baseline hazard function is the hazard function for a subject with all explanatory variables equal to zero. For the models we will discuss, the hazard ratio $v$ is set to equal $exp(\beta)$ since the hazard ratio cannot be a negative value. The parameter $beta$ is thus the log of the hazard ratio, expressed $\beta = log(v)$. Any value of $\beta$ will output a positive value $v$. We can include many explanatory variables in the hazard function such as factors, which can take on different levels, or variates, which can be any value on a continuous scale. With many predictors, the Cox proportional hazards model can be expressed as $h_i(t) = h_0(t)exp(\beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_px_{pi})$, where $h_0(t)$ is the baseline hazard function, $x_1$ to $x_p$ are the values of the explanatory variables for individual $i$, and $\beta_1$ to $\beta_p$ are the regression coefficients. The model can also be expressed as a linear model in terms of the log of the ratios between the two hazard functions, looking like $log(\frac{h_i(t)}{h_0(t)}) = \beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_px_{pi}$ (@collet).

## Method of Maximum Likelihood

In the Cox proportional hazards model, there are two unknowns: the baseline hazard function, $h_0(t)$, and the regression coefficients, $\beta_1$ to $\beta_p$. The method of maximum likelihood is a statistical method used to estimate values for unknown parameters by fitting a model that maximizes the likelihood of outputting the data we have (@allison84). In general, the maximum likelihood estimate for an unknown value, $\theta$, is the value that maximizes the likelihood function, $L(\theta) = \prod_{i=1}^n f(x_i|\theta)$, where $f(x_i|\theta)$ is the probability density function, or pdf, of the data (@pennstate). The pdf and CDF, or the Cumulative Distribution Function, of the data are functions that express probability of certain values based on the distribution of the data. The baseline hazard function, $h_0(t)$, can be expressed as a function of the pdf and CDF of the data, looking like $h_0(t) = \frac{f(t)}{1-F(t)}$, where $f(t)$ is the pdf and $F(t)$ is the CDF (@allison84). However, as mentioned before, a specific distribution of the data is not assumed in Hazard Analysis. This means that the pdf, CDF, and therefore the baseline hazard function, can not be calculated using the method of maximum likelihood. Instead, the method of partial likelihood can be used to estimate the regression coefficients (@collet).

## Method of Partial Likelihood

The method of partial likelihood, first discovered by Cox himself, allows us to estimate the regression coefficients without knowing the baseline hazard function (@collet). As a reminder, the baseline hazard function describes the distribution of the data when no events have happened, but we only have data points at times when either an event occurs or an individual is censored. Thus, we need a way to fit a function that maximizes the likelihood of getting these data points without knowing what happens at every time. The method of partial likelihood does this by ranking the times of events in the data, and then using these ordered event times to predict the hazard ratio. It is called a partial likelihood method because it does not actually use the exact times of events but instead just their rankings (@collet). One key assumption of this method is that there are no ties in the data, or events at the same time. Additionally, it is important to note that this method assumes that the time intervals between each event are independent of the model parameters, or that the time intervals give no information about the model parameters (@waagepetersen22).

## Partial Likelihood Derivation

The method of partial likelihood is based on the assumption that the time intervals between events gives no information about the model parameters. Instead, the method uses the order of the events to estimate the hazard ratio. The method uses the idea of conditional probability that a certain individual has an event at time $t_j$ given that they have survived until time $t_i$ and that an event has occurred at time $t_j$.

The definition of conditional probability is given by: $P(A|B) = \frac{P(A ∩ B)}{P(B)}$. This is saying that the probability of some event A given that an event B has occurred is equal to the probability that both events A and B occur divided by the probability that event B occurs. In the context of the Cox proportional hazards model, the conditional probability would look like: $P(\text{individual with variables } x_i \text{ experiences event at time } t_j | \text{one event at time } t_j )$ = $\frac{P(\text{individual with variables } x_i \text{ experiences event at time } t_j)}{P(\text{an event happens at time } t_j)}$.

The probability of an event happening at $t_j$ can be represented numerically as the sum of the probabilities of event for all of the individuals at risk at time $t_j$, or ${\sum_{l \in R(t_j)} P(\text{individual l dies at time j}})$. Now, we can replace these terms with the hazard functions, $h_i(t_j)$ and $h_lt_j)$, for the numerator and denominator, making the new probability $\frac{h_i(t_j)}{\sum_{l \in R(t_j)} h_l(t_j)}$ (@collet).

Recall that the hazard ratio, $h_i(t)$, is defined as the hazard function of individual i at time t, or $h_i(t) = h_0(t)exp(\beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_px_{pi})$. The baseline hazard function, $h_0(t)$, will thus cancel out in the conditional probability equation, simplifying it to $\frac{exp(\beta' x_j)}{\sum_{l \in R(t_j)} exp(\beta' x_l)}$, where $\beta'$ is the vector of regression coefficients and $x_j$ is the vector of predictor variables for individual j (@collet).

The partial likelihood method states that the likelihood of the data is the product of the conditional probabilities of the events happening at the ordered event times, or $L(\beta) = \prod_{j=1}^r \frac{exp(\beta' x_j)}{\sum_{l \in R(t_j)} exp(\beta' x_l)}$, where there are a total of r ordered event times and j is the index of the ordered event times. $R(t_j)$ is the set of individuals at risk at time $t_j$, and $l$ is the index of the individual within the risk set at time j. This is the likelihood of the data given the regression coefficients, $\beta'$.

The partial likelihood function can then be written as the natural logarithm of the likelihood function, or $log(L(\beta)) = log(\prod_{j=1}^r \frac{exp(\beta' x_j)}{\sum_{l \in R(t_j)} exp(\beta' x_l)})$. The goal of the method of partial likelihood is to find the values of $\beta'$ that maximize the partial likelihood function (@collet). Once we fit this equation, we can estimate these parameters by taking the derivative with respect to $\beta'$ and setting it the equation equal to zero, or $\frac{\partial l(\beta)}{\partial \beta} = 0$. This will give us the partial likelihood estimate for the regression coefficients.

## Example Data

For the first example, lets only use one predictor variable, group. The data we will use is shown in @tbl-surv4.

```{r}
#| label: tbl-surv4
#| tbl-cap: "Example Data Set with ID, Time, Status, and Group"

surv4 <- surv3 %>% 
  mutate(id = row_number()) %>%
  select(id, time, status, group) %>% 
  arrange(group, time)

surv4 %>%
  gt(caption = "Example Data Set with ID, Time, Status, and Group") %>%
  cols_label(id = "ID", time = "Time", status = "Status", group = "Group")
```

Let's start by calculating a hazard function between groups 1 and 2. To do this, we will assume that the baseline hazard function is constant, or $h_0(t) = h_0$.

Recall that the Cox proportional hazard analysis equation is $h_i(t) = h_0(t)exp(\beta_1x_{1i} + \beta_2x_{2i} + ... + \beta_px_{pi})$, or $h_i(t) = h_0(t)exp(\beta' x_{i})$, where $x_i$ is a binary variable representing what group individual i is in. For group 1 in our example, $h_1(t) = h_0(t)exp(\beta'x_{1})$, and for group 2, $h_2(t) = h_0(t)exp(\beta'x_{2})$.

We can then calculate the hazard ratio between the two groups as $v = \frac{h_1(t)}{h_2(t)} = \frac{h_0(t)exp(\beta' x_{1})}{h_0(t)exp(\beta' x_{2})} = \frac{exp(\beta' x_{1})}{exp(\beta' x_{2})}$ (@collet).

## Censoring

Since censoring is still a concern for our data, an additional term needs to be added to the partial likelihood function, making it now: $L(\beta) = \prod_{i=1}^r [\frac{exp(\beta' x_i)}{\sum_{l \in R(t_i)} exp(\beta' x_l)}]^{δ_i}$, where {δ_i} is an indicator variable that is 0 if the $rth$ event time is right-censored and 1 otherwise. This term is added to the likelihood function to account for the fact that the event time is not observed for right-censored individuals (@collet). When taking the log, the function becomes $log(L(\beta)) = \sum_{i=1}^r δ_i [\beta' x_i - log \sum_{l \in R(t_i)} exp(\beta' x_l)]$ (@collet).

## Ties in the Data

One of the key assumptions we are making is that there are no ties in event or censor times. However, this is not always the case. There are additional models proposed to account for ties in the data, such as the Efron method and the Breslow method, which add weights to the likelihood function to account for ties in the data. Later, we will use the Breslow method in analysis because it is the simplest. Breslow suggested the equation: $L(B) = \prod_{i=1}^{r} \frac{exp(\beta' s_i)}{(\sum_{l \in R(t_i)} exp(\beta' x_l))^{d_i}}$, where $d_i$ is the number of individuals at risk at time $t_i$ (@collet). For now, we will assume there are no ties in the data and use the partial likelihood function without weights.

## Fitting the Partial Log Likelihood Equation

To fit our likelihood function, we must order all of the event times. For group 1, there are only two times of event, time t = 7 for individual 2 and time t = 8 for individual 4. So, we will order the event times as $t_{(1)} < t_{(2)}$, where $t_{(1)}$ represents the event time for individual 2 and $t_{(2)}$ represents the event time for individual 4. Similarly for group 2, there are events at times 1, 3, and 6, so we will order the event times as $t_{(1)} < t_{(2)} < t_{(3)}$, representing the event times for individuals 10, 6, and 9. We will also need to calculate the risk set at each time, which is shown below in @tbl-surv5.

```{r}
#| label: tbl-surv5
#| tbl-cap: "Example Data Set with Status, Group, Time, and Risk Set"

risk <- c(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)

surv5 <- surv4 %>% 
  arrange(time) %>% 
  mutate(risk_set = risk) %>%
  select(id, group, time, status, risk_set) 

surv5 %>%
  gt(caption = "Example Data Set with Status, Group, Time, and Risk Set") %>%
  cols_label(id = "ID", group = "Group", time = "Time", status = "Status", 
             risk_set = "Risk Set")
```

To estimate the parameters, we will need to use the natural log of our partial likelihood function, which we derived as: $log(L(\beta)) = \sum_{i=1}^r δ_i [\beta' x_i - log \sum_{l \in R(t_i)} exp(\beta' x_l)]$.

At time $t=1$, the risk set will include all 10 individuals because at the tie just before t=1, there are no individuals who have experienced the event or been censored. At time $t=2$, the risk set will include all individuals except for individuals with ID 10 because they experienced the event at time 1. We can continue this process for each time, and the risk set will be the same for both groups.

Now that we know the risk sets and the event times, we can calculate the partial likelihood function. Recall that the partial likelihood function is $L(\beta) = \prod_{i=1}^{n} \frac{exp(\beta x_i)}{\sum_{j \in R(t_i)} exp(\beta x_j)}$. So, for our data set, we will need to calculate the partial likelihood function for each event time. For each relevant time, we can calculate the risk score for individual i, $v(i) = B'{x_i}$ (@collet). This risk score represents the numerator of the partial likelihood equation. For time $t = 1$, in which individual 10 experiences the event of interest, the risk score is denoted $v(10)$. The denominator of the partial likelihood equation is the sum of the risk scores for all individuals in the risk set at time $t_i$, so all individuals 1 through 10. The denominator is then $v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)$. The partial likelihood function for time $t = 1$ is then $\frac{v(10)}{v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)}$.

After doing this for each event time, we get the partial likelihood equation to be $\frac{v(10)}{v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)}$ \* $\frac{v(6)}{v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9)}$ \* $\frac{v(9)}{v(2) + v(3) + v(4) + v(8) + v(9)}$ \* $\frac{v(2)}{v(2) + v(3) + v(4) + v(8)}$ \* $\frac{v(4)}{v(3) + v(4) + v(8)}$.

We can then take the log of the partial likelihood function to get the log partial likelihood function: $log(L(B)) = 3\beta x_2 + 2\beta x_1 - log(5exp(\beta x_1) + 5exp(\beta x_2))$ - $log(4exp(\beta x_1) + 4exp(\beta x_2)) - log(3exp(\beta x_1) + 2exp(\beta x_2)) - log(3exp(\beta x_1)$ + $exp(\beta x_2)) - log(2exp(\beta x_1) + exp(\beta x_2))$.

## Estimating B'

We can then take the derivative of the log partial likelihood function with respect to $\beta$ and set it equal to 0 to find the value of $\beta$ that maximizes the partial likelihood function. The derivative will be: $\frac{\partial log(L(\beta))}{\partial \beta}$ = $3x_2 + 2x_1 - \frac{5x_1exp(\beta x_1)}{5exp(\beta x_1) + 5exp(\beta x_2)} - \frac{5x_2exp(\beta x_2)}{5exp(\beta x_1) + 5exp(\beta x_2)} - \frac{4x_1exp(\beta x_1)}{4exp(\beta x_1) + 4exp(\beta x_2)} - \frac{4x_2exp(\beta x_2)}{4exp(\beta x_1) + 4exp(\beta x_2)}$ - $\frac{3x_1exp(\beta x_1)}{3exp(\beta x_1) + 2exp(\beta x_2)} - \frac{2x_2exp(\beta x_2)}{3exp(\beta x_1) + 2exp(\beta x_2)} - \frac{3x_1exp(\beta x_1)}{3exp(\beta x_1) + exp(\beta x_2)} - \frac{x_2exp(\beta x_2)}{3exp(\beta x_1) + exp(\beta x_2)} - \frac{2x_1exp(\beta x_1)}{2exp(\beta x_1) + exp(\beta x_2)}$ - $\frac{x_2exp(\beta x_2)}{2exp(\beta x_1) + exp(\beta x_2)}$.

Setting this equation equal to zero and solving for $\beta$ would theoretically give us the value of $\beta$ that maximizes the partial likelihood function. However, because this equation is not solvable, we can use numerical methods to approximate the value of $\beta$. One of the most common methods is called the Newton Raphson method.

## Newton Raphson Method

The Newton Raphson method is an iterative method that is used to find the root of a function, f(x). In our case, we case use this method to find the value of $\beta$ that maximizes the partial likelihood function. We do this by starting with an initial estimate, ${x_0}$, for the root, and repeating a series of steps to improve this estimation. We will first chose a value for x, $x_0$, and then find the equation of the tangent line to the function at the point $(x_0, f(x_0))$. We can then find the x-intercept of the tangent line at $x_0$ by setting our function equal to 0. This x-intercept is our next estimate, $x_1$, for the root (@anstee).

Let f(x) be the function we want to find the root of and r be the root of the equation when f(x) = 0. Suppose $r = {x_0} + h$. The value of h is the distance from the true root, r, and our initial estimate, $x_0$. A key assumption of this method is that ${x_0}$ is a good estimate and therefore h is close to the true value. Because h is small, we can use the linear approximation of the tangent line. To do this, we know that the slope of the tangent line is the derivative of the function at the point $(x_0, f(x_0))$. Thus we can estimate $0 = f(r) = f({x_0} + h) = f({x_0}) + f'({x_0})h$. Solving for h we get $h = -\frac{f({x_0})}{f'({x_0})}$. Thus, $r = {x_0} + h = {x_0} - \frac{f({x_0})}{f'({x_0})}$. The new value of r will be a better estimate for the root, $x_1$ (@anstee).

The equation used for this iteration looks like: $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$. We can repeat this step until the difference between ${x_n+1}$ and ${x_n}$ is less than a certain tolerance level, meaning that our estimate is not changing much with new iterations (@anstee).

## Newton Raphson Example

Let's consider a simple example of a Newton Raphson iteration. Suppose we want to find the root of the function, $f(x) = x^3 +3x + 1$. First, we will take the derivative of the function, which will be $f'(x) = 3x^2 + 3$. We can then use the Newton Raphson method to find the root of the function. We will start with an initial estimate, $x_0 = 3$, and use the equation $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$ to find our second estimate for the root. In this case, $x_{n+1} = 3 - \frac{f(3)}{f'(3)}$ = $ 3 - \frac{3^3 + 3*3 + 1}{3*3^2 + 3}$ = $3 - \frac{37}{30}$ = $3 - 1.2333$ = $1.7667$. We can then use this value as our new estimate and repeat the process until the difference between ${x_n+1}$ and ${x_n}$ is less than a certain tolerance level. Let's visualize this process in table @tbl-nr1.

```{r}
#| label: tbl-nr1
#| tbl-cap: "Newton Raphson Iteration"

# define function and its derivative
f = function(x) x^3 + 3*x + 1
fprime = function(x) 3*x^2 + 3

# Choose initial estimate to be 3
x = 3

# Use tolerance level of 0.0001
tol = 0.0001

# Initialize a vector to store the results
iterations <- c(x)

# While loop to repeat the process until the difference between ${x_n+1}$ and ${x_n}$ is less than the tolerance level
while (abs(f(x)) > tol){
  x = x - f(x)/fprime(x)
  x = round(x, 4)
  iterations <- c(iterations, x)  # Store the current value of x
}

# Create a data frame with results
results <- data.frame(iteration = 1:length(iterations), x = iterations)
results %>%
  gt(caption = "Newton Raphson Iteration") %>%
  cols_label(iteration = "Iteration", x = "x")
```

The table shows the results of the Newton Raphson method. We can see that the value of x is converging to the root of the function, which is approximately -0.32. 

## Newton Raphson for Parametized Functions: Gamma Distribution

The above example shows the general process for the Newton Raphson iteration, but what about when we have a function with multiple parameters? Suppose we want to find the root of the gamma distribution, $f(x) = \frac{B^a}{\Gamma(a)}x^{a-1}e^{-Bx}$. First, we will take the natural log of the function and then take its derivative with respect to both a and B. We will then use the Newton Raphson method to find the values of B and a that maximizes the function.

We would start with an estimate for the root by using other methods such as Method of Moments Estimation. The Method of Moments Estimator of the parameters of the Gamma Distribution are: $\hat{a} = \frac{\bar{x}^2}{s^2}$ and $\hat{B} = \frac{\bar{x}}{s^2}$, where $\bar{x}$ is the sample mean and s is the sample standard deviation. We can then use these estimates as our initial estimates for the Newton Raphson method using the equation $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$ to find our second estimate for the root and then repeating the process until our tolerance requirement is met. 

```{r}
set.seed(1)
gamma<- round(rgamma(10, shape = 3, rate = .5), 1)

sum(gamma)
sum(log(gamma))
mean(gamma)
var(gamma)
```

```{r}
#| label: tbl-nr2
#| tbl-cap: "Newton Raphson for Gamma Distribution"

# Define functions
afunc = function(a, L, n, sum_ln_x) a - ((n*log(L) - n * digamma(a) + sum_ln_x) / (-n * trigamma(a)))
Lfunc = function(a, L, n, sum_x) L - (((n*a/L) - sum_x) / (-n * a / L^2))

# Choose initial estimates
a = 3.5
L = .7
n = 10
sum_ln_x = sum(log(gamma))
sum_x = sum(gamma)

# Use tolerance level of 0.1
tol = 0.1

# Initialize a vector to store the results
iterations <- c(a, L)

# While loop to repeat the process until the difference between a and a1 and L and L1 is less than the tolerance level
while (TRUE) {
  a1 <- afunc(a, L, n, sum_ln_x)
  L1 <- Lfunc(a, L, n, sum_x)
  if (abs(a - a1) < tol & abs(L - L1) < tol) {
    break
  }
  a <- a1
  L <- L1
  iterations <- c(iterations, a, L)  # Store the current values of a and L
}

# put the results into a table with every 10th value
results <- data.frame(iteration = 1:length(iterations), 
                      a = iterations[seq(1, length(iterations), 2)], 
                      L = iterations[seq(2, length(iterations), 2)])

# print the table
results %>%
  gt(caption = "Newton Raphson for Gamma Distribution") %>%
  cols_label(iteration = "Iteration", a = "a", L = "L")
```

As we can see, we didn't quite reach the original values of a = 3 and L = 0.5, but with a tolerance of 0.1, we got close. Usually, a lower tolerance is used and many more iterations are needed, but this example demonstrates a similar process to how our statistical software will find the maximum likelihood estimates for our parameters. 

## Hazard Analysis in R

Similarly to before with the Kaplan Meier curve, we can use R to model the hazard function. The `coxph()` function in the `survival` package returns the coefficients of the cox proportional hazards model as well as the p value for the coefficients, allowing us to determine whether each coefficient is significant (@sthda). 

```{r}
coxph(Surv(time, status) ~ group, data = surv3, ties = "breslow")
```

Let's add another predictor and see what happens: 

```{r}
coxph(Surv(time, status) ~ group + age, data = surv3, ties = "breslow")
```
After adding age as a predictor, we see that the coefficient on the age predictor is significant, but the coefficient for group is not. This means that, when controlling for the other variables, the hazard of the event occurring is not significantly different between the two groups, but the hazard of the event occurring is significantly different for individuals of different ages.

## Case Study 1

Let's look back at the Cirrhosis data set.

```{r}
# Load in the data and select the variables we need
cirrhosis <- read_csv("data/cirrhosis.csv", show_col_types = FALSE)
cirrhosis <- cirrhosis %>% 
  mutate(event = if_else(Status == 'D', 1, 0)) %>% 
  select(event, Drug, N_Days)
```

Recall that we can look at the times using the `Surv()` function.

```{r}
head(Surv(cirrhosis$N_Days, cirrhosis$event))
```

Let's see what the Cox Analysis will tell us about the relationship between the drug and the number of days until death.

```{r}
s4 <- coxph(Surv(N_Days, event) ~ Drug, data = cirrhosis, ties = "breslow")
summary(s4)
```

The p value for the drug is not less than 0.05, so we can not conclude that the drug has a significant effect on the number of days until death.

## Case Study 2

Let's look at a new data set which contains information on patients with heart failure.

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181001#sec005

```{r}
# Load in dataset
heart <- read_csv("data/S1Data.csv")
```

First, lets view the firt 6 survival times. 

```{r}
# View some of the times
head(Surv(heart$TIME, heart$Event))
```

Let's run a con proportional hazards model to see if blood pressure is a significant predictor of the hazard of death for heart failure patients. 

```{r}
h1 <- coxph(Surv(TIME, Event) ~ BP, data = heart, ties = "breslow")
summary(h1)
```

As we can see, the p-value on the blood pressure variable is significant. Thus, we can conclude that high blood pressure is significantly associated with a higher hazard of death for heart failure patients. The coefficient on blood pressure in the model is 0.44, which means that the hazard of death for heart failure patients with high blood pressure is estimated to be 0.44 times higher than the hazard of death for heart failure patients with normal blood pressure.

Let's look at a confidence interval to get a better understand of the impact of high blood pressure on the hazard of death for heart failure patients. We can use the `confint()` function to get a 95% confidence interval for the coefficient of blood pressure.

```{r}
conf_95 <- confint(h1)
conf_95
```

This gives us a 95% confidence interval for the coefficient of blood pressure. It tells us that, with 95% confidence, the true value of the coefficient is between 0.025 and 0.85. That is, that the hazard of death for heart failure patients is between 0.025 and 0.85 times higher for patients with high blood pressure compared to patients with normal blood pressure.

## References
