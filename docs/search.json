[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survival Analysis Senior Project",
    "section": "",
    "text": "1 Survival Analysis"
  },
  {
    "objectID": "index.html#survival-analysis-background",
    "href": "index.html#survival-analysis-background",
    "title": "Survival Analysis Senior Project",
    "section": "1.1 Survival Analysis Background",
    "text": "1.1 Survival Analysis Background\nSurvival analysis is a type of statistical analysis used for analyzing the relationship between time and an event of interest occurring for individuals. The name survival analysis implies that the most common outcome analyzed is death, but many other events can be analyzed including: time it takes to begin recovering from treatment or time until a disease is contracted. These analyses are specifically helpful when comparing groups of people such as treatment groups in a clinical trial. Survival analysis can reveal whether certain treatments are more effective than others in helping individuals to live longer or avoid certain outcomes of interest, such as heart attacks."
  },
  {
    "objectID": "index.html#example-data-set-in-r",
    "href": "index.html#example-data-set-in-r",
    "title": "Survival Analysis Senior Project",
    "section": "1.2 Example data set in R",
    "text": "1.2 Example data set in R\nTo demonstrate survival analysis, an example data set was created including ten observations with the following columns: a. \\(id\\) variable for each observation, a \\(time\\) variable ranging from 1 to 10, and a \\(status\\) variable that was either a 1, which represented the event occurrence, or a 0, representing no event occurrence. For those assigned a 1, a survival time was assigned in the range of 1 to 9, representing the time that individual survived until the event occurred for them. For those assigned a 0, a time of t = 10 was assigned, suggesting that they lasted until the end of the hypothetical study without the event occurring. Table 1.1 shows these data.\n\n# Load Packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(survival)\nlibrary(ggsurvfit)\nlibrary(gt)\n\n\n# Create a simple data set\nid &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ntime &lt;- c(1, 2, 4, 5, 6, 8, 9, 10, 10, 10)\nstatus &lt;- c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0)\nsurv &lt;- data.frame(id, time, status)\n\n\nsurv %&gt;% gt(caption = \"Example Data Set with ID, Status, and Time\") %&gt;%\n  cols_label(id = \"ID\", time = \"Time\", status = \"Status\") %&gt;%\n  tab_source_note(source_note = \"Table 1\") \n\n\n\n\n\n\n  Table 1.1:  surv table 1 \n  \n    \n    \n      ID\n      Time\n      Status\n    \n  \n  \n    1\n1\n1\n    2\n2\n1\n    3\n4\n1\n    4\n5\n1\n    5\n6\n1\n    6\n8\n1\n    7\n9\n1\n    8\n10\n0\n    9\n10\n0\n    10\n10\n0\n  \n  \n    \n      Table 1\n    \n  \n  \n\n\n\n\n\nTo perform survival analysis on these data, the probability of survival at each time from t = 0 to t = 10 can be calculated by dividing the amount of people surviving until that time by the total amount of people in the study. For the first observation in the example data, probability of survival is equal to 1, or 100%, because every participant would presumably begin the study alive. To calculate the probability of survival for time t = 1, the number of participants that did not experience the outcome of interest by t = 1 would be divided by 10, the total number of observations in the study. In this case, 9 participants survived until time t = 1, because only one observation was assigned a 1 between times t = 0 and t = 1. Dividing this total by the 10 total observations in the study shows that the probability of surviving to time 1 is 0.9, or 90%. As time increases, the total probability of survival for the group will decrease in the range of 1 to 0 because more people will be experiencing the outcome. This is why survival curves have a rightly skewed distribution. Table 1.2 shows the survival probabilities for each observation.\n\n# Calculate survival probabilities\nprobabilities &lt;- data.frame(\n  Number_alive = c(10, 9, 8, 8, 7, 6, 5, 4, 3, 3, 3),\n  Time = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n  Probability = c(1, 0.9, 0.8, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.3, 0.3))\n\n\nprobabilities %&gt;% gt(caption = \"Probability of Survival Based on Time\") %&gt;%\n  cols_label(Number_alive = \"Number alive\", Time = \"Time\", Probability = \"Probability\") %&gt;%\n  tab_source_note(source_note = \"Table 2\") \n\n\n\n\n\n\n  Table 1.2:  probabilities \n  \n    \n    \n      Number alive\n      Time\n      Probability\n    \n  \n  \n    10\n0\n1.0\n    9\n1\n0.9\n    8\n2\n0.8\n    8\n3\n0.8\n    7\n4\n0.7\n    6\n5\n0.6\n    5\n6\n0.5\n    4\n7\n0.4\n    3\n8\n0.3\n    3\n9\n0.3\n    3\n10\n0.3\n  \n  \n    \n      Table 2"
  },
  {
    "objectID": "index.html#censoring-background",
    "href": "index.html#censoring-background",
    "title": "Survival Analysis Senior Project",
    "section": "1.3 Censoring Background",
    "text": "1.3 Censoring Background\nOne issue with survival analysis data is the problem of an individual being lost-to-follow-up, meaning data could not be collected for them at some point during the study. This causes the outcome of interest to not be recorded for that individual, so the survival time for that individual can not be analyzed as it would not be accurate. When this happens, the survival times for those individuals are recorded as censored, causing standard analysis techniques to be inappropriate for these data. There are three types of censoring that can occur. The first type of censoring is called right censoring, which happens if an individual begins the study but then is lost-to-follow-up at some point during the study before it ends. The censored survival time for these individuals is thus equal to the total time they were known to be alive in the study’s observation period before they were lost-to-follow-up. Some common examples of when right censoring is needed include: an individual moving away from the study and not being able to participate or when an individual dies due to a non-related event after the study begins. Another type of censoring is called left censoring. In this type, the individual experiences lost-to-follow-up before the observation period begins. This would happen, for example, in a study that tracks patient recovery from a surgery, but with an observation period beginning one a month after the surgery took place. If a patient died less than a month after surgery, their survival time would need to be left censored. The final type of censoring is called interval censoring, which happens when a patient comes in and out of the study, making it possible for them to experience the outcome of interest during a period of time when they aren’t being observed. This often happens when recurrence is being tracked in a study. One example could be if recurrence of cancer is being tracked and the study checks in with patients every month. If an individual does not have cancer after the first month but then does after the second month, the recurrence time is somewhere between one and two months, and it therefore needs to be interval censored (Collet (2003)). Out of the three types, right censoring is the most common and will be demonstrated in the example data-set."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Survival Analysis Senior Project",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nCollet, David. 2003. Modelling Survival Data in Medical Research. Chapman & Hall/CRC."
  },
  {
    "objectID": "KaplanMeier.html#kaplan-meier-survival-estimate",
    "href": "KaplanMeier.html#kaplan-meier-survival-estimate",
    "title": "2  Kaplan Meier",
    "section": "2.1 Kaplan Meier Survival Estimate",
    "text": "2.1 Kaplan Meier Survival Estimate\nThe Kaplan Meier survival estimate is how survival probabilities can be calculated while factoring in censored times. The Kaplan Meier survival estimate is used to calculate probability of survival at a given time where \\(S(t_j)\\) is the probability of being alive at time \\(t_j\\), \\(S(t_{j-1})\\) is the probability of being alive at \\(t_{j-1}\\), \\(n_j\\) is the number of patients alive just before \\(t_j\\), \\(d_j\\) is the number of events at \\(t_j\\), and \\(j\\) is the time interval of interest. The equation is \\(S(t_j)=S(t_{j-1})(1-\\frac{d_j}{n_j})\\) (Clark (2003)). The equation essentially divides the surviving individuals by the individuals at risk, similar to the previous calculations shown. However, Kaplan Meier curves adjust for right-censored times by dropping observations from the total number of individuals at risk, \\(n_j\\), after their censored time has been reached. This adjustment prevents overestimating the survival probability because it no longer assumes censored individuals are still alive or and at risk (Goel (2010))."
  },
  {
    "objectID": "KaplanMeier.html#kaplan-meier-curve-and-descriptive-statistics",
    "href": "KaplanMeier.html#kaplan-meier-curve-and-descriptive-statistics",
    "title": "2  Kaplan Meier",
    "section": "2.2 Kaplan Meier Curve and Descriptive Statistics",
    "text": "2.2 Kaplan Meier Curve and Descriptive Statistics\nThe Kaplan Meier curve is a graphical representation of survival analysis. Similar to the survival probabilities discussed previously, the Kaplan Meier curve shows the relationships between time, which is typically plotted on the x-axis, and probability of survival, which is typically on the y-axis. The curve always ranges from 0 to 1 and is right skewed. Useful summary statistics for a Kaplan Meier Survival curve include confidence intervals and the median. A 95% confidence interval for each point can be found by using the formula \\(S_t ± 1.96 * SE(S_t)\\). The median value is typically reported rather than the mean because mean survival time cannot be reported reliably for those who have not experienced the outcome of interest yet. The median can be found by finding the time when probability of survival is equal to 0.5. Thus, the median can only be reported when at least half of the participants experienced the outcome of interest during the study (Rao (2023))."
  },
  {
    "objectID": "KaplanMeier.html#kaplan-meier-in-r",
    "href": "KaplanMeier.html#kaplan-meier-in-r",
    "title": "2  Kaplan Meier",
    "section": "2.3 Kaplan Meier in R",
    "text": "2.3 Kaplan Meier in R\nLuckily, modern software makes these calculations easy and fast, as well as plotting them with confidence intervals and risk tables. A new data set with censored times will be created to demonstrate this process. To do this, a 0 will be recorded for some individuals at times before t = 10. Table 2.1 shows this data set’s structure.\n\n# Load Packages\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(survival)\nlibrary(ggsurvfit)\nlibrary(gt)\n\n\n# Create data set\nid &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n#id &lt;- c(5, 5, 7, 7, 8, 8, 9, 9, 1, 1)\ntime &lt;- c(1, 2, 10, 4, 5, 6, 10, 8, 9, 10)\nstatus &lt;- c(1, 1, 0, 0, 1, 0, 0, 1, 1, 0)\ncensor &lt;- data.frame(id, time, status)\n\n\ncensor %&gt;% gt(caption = \"Example Data Set with Censored Times\") %&gt;%\n  cols_label(id = \"ID\", time = \"Time\", status = \"Status\") %&gt;%\n  tab_source_note(source_note = \"Table 3\") \n\n\n\n\n\n\n  Table 2.1:  Example Data Set with Censored Times \n  \n    \n    \n      ID\n      Time\n      Status\n    \n  \n  \n    1\n1\n1\n    2\n2\n1\n    3\n10\n0\n    4\n4\n0\n    5\n5\n1\n    6\n6\n0\n    7\n10\n0\n    8\n8\n1\n    9\n9\n1\n    10\n10\n0\n  \n  \n    \n      Table 3\n    \n  \n  \n\n\n\n\n\nThe survival package in R has a function called Surv() that takes input data and creates a response object recording survival time for each observation. The function takes in the time variable and the status variable. The function takes into account right censoring as well, marking censored times with a \\(+\\) symbol in the object created (Zabor (2023)).\n\n# Show the time of event or censored time for each observation\ntimes &lt;- Surv(censor$time, censor$status)\ntimes\n\n [1]  1   2  10+  4+  5   6+ 10+  8   9  10+\n\n\nThe survfit() function can be used to calculate the Kaplan Meier survival estimate for each time that a new event occurs. The function takes in the response object created by the Surv() function in order to drop censored observations from the regression. We use ~ 1 because we are not including predictor variables in the model. The function returns a survival object that can be used to plot the Kaplan Meier curve and calculate summary statistics. The time variable in the object created by the survfit() function shows the time of each event, and the surv variable shows the survival probability for the remaining individuals after each event occurs.\n\n# Calculate survival object\ns1 &lt;- survfit(times ~ 1, data = censor)\n# View the survival time for each time an event occurs\ns1$time\n\n[1]  1  2  4  5  6  8  9 10\n\n# View survival probability for remaining individuals after each event occurs \nround(s1$surv, 2)\n\n[1] 0.90 0.80 0.80 0.69 0.69 0.55 0.41 0.41\n\n\nThe ggsurvfit package can be used to plot the Kaplan Meier curve for this data using the previously created s1 object. Figure 2.1 shows the Kaplan Meier curve for predicting the event. The risk table shows the number of individuals at risk at each time point and the number of events that occurred at each time point.\n\ns1 %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Time\",\n    y = \"Overall survival probability\",\n    title = \"Kaplan Meier Survival Curve\"\n  ) + \n  scale_x_continuous(breaks=seq(0,10,by=2)) + \n  scale_y_continuous(breaks=seq(0,1,by=.2)) +\n  add_risktable()\n\n\n\n\nFigure 2.1: Kaplan Meier Survival Curve"
  },
  {
    "objectID": "KaplanMeier.html#log-rank-test",
    "href": "KaplanMeier.html#log-rank-test",
    "title": "2  Kaplan Meier",
    "section": "2.4 Log-Rank Test",
    "text": "2.4 Log-Rank Test\nOne of the most common applications of survival analysis and Kaplan Meier curves is for comparing survival times between two groups. One example of when this might be useful is if a study is comparing survival after two different treatment plans. A data set with two groups will be created to demonstrate this process.\n\n# Create new data set\ntime &lt;- c(8, 7, 10, 8, 5, 3, 4, 10, 6, 1)\nstatus &lt;- c(0, 1, 0, 1, 0, 1, 0, 0, 1, 1)\ngroup &lt;- c(1, 1, 1, 1, 1, 2, 2 ,2, 2, 2)\nsurv2 &lt;- data.frame(time, status, group)\n\nTo run analysis, the statistical test called the Log-Rank Test can be used to test the null hypothesis that there is no difference between the survival estimates of two groups at any point in time (Rich (2010)). The test is conducted by comparing the observed number of event with an estimated number of events for each group at each time. To model this, we need to record some values of interest from our existing data. The first, \\(n1t\\), represents the number of people alive and still in the study, or at risk, at time t in group 1. Similarly, \\(n2t\\) is the number at risk in group 2 at time t. \\(nt\\) is the sum of these two values at time t, representing the total number of people at risk in the study at time t. \\(o1t\\) is the number of observed events occurring at time t in group 1, \\(o2t\\) is observed events occurring at time t in group 2, and \\(ot\\) is the total observed events across both groups at time t. To calculate expected number of events, the assumption that the two curves are identical is used. The expected number of events at a time t is for group i calculated by the formula \\(E_{it} = \\frac{N_{it} \\times O_{t}}{N_{t}}\\), where \\(N_{it}\\) is the observed number of events in group i at time t, \\(O_{t}\\) is the total number of events in all groups at time t, and \\(N_{t}\\) is the total number observations at risk in all groups at time t (Sullivan (2016)). Table 2.2 shows the creation of all of these values, where \\(e1t\\) is the expected number of events at time t in group 1, \\(e2t\\) is the expected number of events at time t in group 2, and \\(et\\) is the total expected number of events across both groups at time t. Cumulative sums are also calculated for each of these values.\n\nTime &lt;- c(1, 3, 4, 5, 6, 7, 8, 10)\nN1t &lt;- c(5, 5, 5, 5, 4, 4, 3, 1)\nN2t &lt;- c(5, 4, 3, 2, 2, 1, 1, 1)\nO1t &lt;- c(0, 0, 0, 0, 0, 1, 1, 0)\nO2t &lt;- c(1, 1, 0, 0, 1, 0, 0, 0)\n\nlog_rank &lt;- data.frame(Time, N1t, N2t, O1t, O2t)\n\n# Calculate Totals\nlog_rank$Nt &lt;- log_rank$N1t + log_rank$N2t\nlog_rank$Ot &lt;- log_rank$O1t + log_rank$O2t\nlog_rank$E1t &lt;- (log_rank$N1t * log_rank$Ot) / log_rank$Nt\nlog_rank$E2t &lt;- (log_rank$N2t * log_rank$Ot) / log_rank$Nt\n\nround(log_rank, 2) %&gt;% \n  gt() %&gt;% \n  tab_header(\"Values Needed to Calculate Chi-Square Statistic\") %&gt;% \n  cols_label(Time = \"Time\", N1t = \"Number at Risk in Group 1\", N2t = \"Number at Risk in Group 2\", Nt = \"Total Number at Risk\", O1t = \"Observed Events in Group 1\", O2t = \"Observed Events in Group 2\", Ot = \"Total Observed Events\", E1t = \"Expected Events in Group 1\", E2t = \"Expected Events in Group 2\")\n\n\n\n\n\nTable 2.2:  Calculation of Log-Rank Test Statistic \n  \n    \n      Values Needed to Calculate Chi-Square Statistic\n    \n    \n    \n      Time\n      Number at Risk in Group 1\n      Number at Risk in Group 2\n      Observed Events in Group 1\n      Observed Events in Group 2\n      Total Number at Risk\n      Total Observed Events\n      Expected Events in Group 1\n      Expected Events in Group 2\n    \n  \n  \n    1\n5\n5\n0\n1\n10\n1\n0.50\n0.50\n    3\n5\n4\n0\n1\n9\n1\n0.56\n0.44\n    4\n5\n3\n0\n0\n8\n0\n0.00\n0.00\n    5\n5\n2\n0\n0\n7\n0\n0.00\n0.00\n    6\n4\n2\n0\n1\n6\n1\n0.67\n0.33\n    7\n4\n1\n1\n0\n5\n1\n0.80\n0.20\n    8\n3\n1\n1\n0\n4\n1\n0.75\n0.25\n    10\n1\n1\n0\n0\n2\n0\n0.00\n0.00\n  \n  \n  \n\n\n\n\n\nTo test the null hypothesis that there is no difference between the survival estimates of two groups at any point in time, a test statistics is needed. For the Log-Rank test, a Chi-Square test statistic is used because the data follows a Chi-Square curve rather than a normal distribution. The Chi-Square test statistic is calculated by the formula \\(X^2 = \\sum_{i=1}^k \\frac{(\\sum{O_{it}} - \\sum{E_{it}})^2}{\\sum{Vi}}\\), where \\(\\sum_{0}^T{O_{it}}\\) is the sum of the observed number of events in group i over the entire time interval and \\(\\sum_{0}^T{E_{it}}\\) is the sum of the expected number of events in group i over the entire time interval. The difference of the sums of these two values are divided by the sum of the variance. The variance for group 1 at time t is calculated by the formula \\(V_{1t} = \\frac{N_{1t} \\times N_{2t} \\times O_{t} \\times (N_{t} - O_{t})}{N_{t}^2 \\times (N_{t} - 1)}\\) (Collet (2003)). The variance for group 2 is calculated similarly, and the total variance is calculated by the formula \\(V_{t} = V_{1t} + V_{2t}\\). Table 2.3 shows the calculation of the Chi-Square test statistic for the data set. In the table, \\(V1t\\) is the variance of the number of events at time t in group 1, \\(V2t\\) is the variance of the number of events at time t in group 2, \\(Vt\\) is the total variance of the number of events across both groups at time t, \\(sum_{Vt}\\) is the cumulative sum of the total variance across all times, and \\(X2\\) is the Chi-Square test statistic.\n\nlog_rank$V1t &lt;- (log_rank$N1t * log_rank$N2t * log_rank$Ot * (log_rank$Nt - log_rank$Ot)) / (log_rank$Nt^2 * (log_rank$Nt - 1))\n\nlog_rank$V2t &lt;- (log_rank$N1t * log_rank$N2t * log_rank$Ot * (log_rank$Nt - log_rank$Ot)) / (log_rank$Nt^2 * (log_rank$Nt - 1))\n\nround(log_rank, 2) %&gt;% \n  gt() %&gt;% \n  tab_header(\"Calculation of Variance\") %&gt;% \n  cols_label(Time = \"Time\", N1t = \"Number at Risk in Group 1\", N2t = \"Number at Risk in Group 2\", Nt = \"Total Number at Risk\", O1t = \"Observed Events in Group 1\", O2t = \"Observed Events in Group 2\", Ot = \"Total Observed Events\", E1t = \"Expected Events in Group 1\", E2t = \"Expected Events in Group 2\", V1t = \"Variance of Number of Events in Group 1\", V2t = \"Variance of Number of Events in Group 2\")\n\n\n\n\n\nTable 2.3:  Calculation of Variance \n  \n    \n      Calculation of Variance\n    \n    \n    \n      Time\n      Number at Risk in Group 1\n      Number at Risk in Group 2\n      Observed Events in Group 1\n      Observed Events in Group 2\n      Total Number at Risk\n      Total Observed Events\n      Expected Events in Group 1\n      Expected Events in Group 2\n      Variance of Number of Events in Group 1\n      Variance of Number of Events in Group 2\n    \n  \n  \n    1\n5\n5\n0\n1\n10\n1\n0.50\n0.50\n0.25\n0.25\n    3\n5\n4\n0\n1\n9\n1\n0.56\n0.44\n0.25\n0.25\n    4\n5\n3\n0\n0\n8\n0\n0.00\n0.00\n0.00\n0.00\n    5\n5\n2\n0\n0\n7\n0\n0.00\n0.00\n0.00\n0.00\n    6\n4\n2\n0\n1\n6\n1\n0.67\n0.33\n0.22\n0.22\n    7\n4\n1\n1\n0\n5\n1\n0.80\n0.20\n0.16\n0.16\n    8\n3\n1\n1\n0\n4\n1\n0.75\n0.25\n0.19\n0.19\n    10\n1\n1\n0\n0\n2\n0\n0.00\n0.00\n0.00\n0.00\n  \n  \n  \n\n\n\n\n\nFor group 1 at time 1, we would calculate \\(E_{1t} = \\frac{5 \\times 1}{10} = 0.50\\) and for group 2 at time 1, \\(E_{2t} = \\frac{5 \\times 1}{10} = 0.50\\). We would repeat for each group at each time and then sum the values to get \\(\\sum_{t=1}^t{E_{i}}\\) for each group. For group 1, \\(\\sum_{t=1}^t{E_{1}} = 0.50 + 0.56 + 0 + 0 + 0.67 + 0.80 + 0.75 + 0 = 3.27\\) and for group 2, \\(\\sum_{t=0}^t{E_{2}} = 0.50 + 0.44 + 0 + 0 + 0.33 + 0.20 + 0.25 + 0 = 1.73\\). The sum of the observed events for group 1 is 2 and for group 2 is 3. For group 1 at time 1, we would calculate \\(V_1t = \\frac{5 \\times 5 \\times 1 \\times (10-1)}{10^2 \\times (10-1)} = 0.25\\) Table 2.4 shows the rest of the variances calculated for both groups for the surv2 data set.\n\nsum_O1t &lt;- cumsum(log_rank$O1t)\nsum_O2t &lt;- cumsum(log_rank$O2t)\nsum_E1t &lt;- cumsum(log_rank$E1t)\nsum_E2t &lt;- cumsum(log_rank$E2t)\nsum_V1t &lt;- cumsum(log_rank$V1t)\nsum_V2t &lt;- cumsum(log_rank$V2t)\n\n# Create table with each of the last values \nsum_stats &lt;- data.frame(sum_O1t, sum_O2t, sum_E1t, sum_E2t, sum_V1t, sum_V2t)\n# Get the last row of summary statistics\nsum_stats &lt;- sum_stats[nrow(sum_stats), ]\nsum_stats$X2_group1 &lt;- ((sum_stats$sum_O1t - sum_stats$sum_E1t)^2 / sum_stats$sum_V1t)\nsum_stats$X2_group2 &lt;- ((sum_stats$sum_O2t - sum_stats$sum_E2t)^2 / sum_stats$sum_V2t)\n\nround(sum_stats, 2) %&gt;% \n  gt() %&gt;% \n  tab_header(\"Calculation of Chi Square Statistic\") %&gt;% \n  cols_label(sum_O1t = \"Sum of Observed Events in Group 1\", sum_O2t = \"Sum of Observed Events in Group 2\", sum_E1t = \"Sum of Expected Events in Group 1\", sum_E2t = \"Sum of Expected Events in Group 2\", sum_V1t = \"Sum of Variance of Number of Events in Group 1\", sum_V2t = \"Sum of Variance of Number of Events in Group 2\", X2_group1 = \"Chi Square Statistic for Group 1\", X2_group2 = \"Chi Square Statistic for Group 2\")\n\n\n\n\n\nTable 2.4:  Calculation of Chi Square Statistic \n  \n    \n      Calculation of Chi Square Statistic\n    \n    \n    \n      Sum of Observed Events in Group 1\n      Sum of Observed Events in Group 2\n      Sum of Expected Events in Group 1\n      Sum of Expected Events in Group 2\n      Sum of Variance of Number of Events in Group 1\n      Sum of Variance of Number of Events in Group 2\n      Chi Square Statistic for Group 1\n      Chi Square Statistic for Group 2\n    \n  \n  \n    2\n3\n3.27\n1.73\n1.07\n1.07\n1.52\n1.52\n  \n  \n  \n\n\n\n\n\nWith all of these values, we can calculate the Chi Square test statistic, \\(X^2i = \\frac{(\\sum{O_{it}} - \\sum{E_{it}})^2}{V_{it}}\\). We get \\(X^2group1 = \\frac{(2 - 3.27)^2}{1.07} = 1.51\\) and \\(X^2group1 = \\frac{(3 - 1.73)^2}{1.07} = 1.51\\). The test statistic can then be compared to a Chi-Square distribution with one k-1 degrees of freedom, with k being the number of groups. So, for this example, there is one degree of freedom. If the p-value is less than 0.05, then we can reject the null hypothesis that there is no difference between the survival estimates of the two groups (Sullivan (2016)).\nClearly, calculating the test statistic is very tedious, even for a data set with 10 observations. The survdiff function can be used to run a Log-Rank test in R, making it much easier to run the test. The function takes in the response object created by the Surv() function and the grouping variable (Zabor (2023)). It returns a Chi-Square statistic and a p-value. We can see that the p-value is much higher than 0.05, indicating that we do not have evidence to reject the null hypothesis, and thus that there is no difference between the survival estimates of the two groups.\n\n# Calculate times for events in this data set\ntimes2 &lt;- Surv(surv2$time, surv2$status)\n# Run a log rank test based on group variable\ntest_stat &lt;- survdiff(times2 ~ group, data = surv2)\ntest_stat\n\nCall:\nsurvdiff(formula = times2 ~ group, data = surv2)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\ngroup=1 5        2     3.27     0.495      1.52\ngroup=2 5        3     1.73     0.937      1.52\n\n Chisq= 1.5  on 1 degrees of freedom, p= 0.2 \n\n\nThe only adjustment needed for plotting the Kaplan Meier curves for two groups is to include the grouping variable while making the survival object using the survfit function (Zabor (2023)). The two survival curves are depicted in Figure 2.2.\n\n# Create survival object using group as a predictor\ns2 &lt;- survfit(Surv(time, status) ~ group, data = surv2)\n\n# Plot the two curves\ns2 %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\",\n    title = \"Kaplan Meier Survival Curve by Group\"\n  ) + \n  scale_x_continuous(breaks=seq(0,10,by=2)) + \n  scale_y_continuous(breaks=seq(0,1,by=.2)) +\n  add_risktable()\n\n\n\n\nFigure 2.2: Kaplan Meier Survival Curve by Group"
  },
  {
    "objectID": "KaplanMeier.html#case-study",
    "href": "KaplanMeier.html#case-study",
    "title": "2  Kaplan Meier",
    "section": "2.5 Case Study",
    "text": "2.5 Case Study\nThe next example uses a data set with variables for predicting death in patients with cirrhosis, which is permanent scarring of the liver. The data set is from a clinical trial conducted by the Mayo Clinic from 1974 to 1984. The data set contains 424 primary biliary cirrhosis patients with 20 variables (Fedesoriano (2021)). To conduct survival analysis, the Status variable needs to be transformed into an indicator variable, labelled event, coded as a 1 representing death and 0 representing censored. The N_Days variable will be used for the time variable, indicating number of days since the beginning of the trial.\n\ncirrhosis &lt;- read_csv(\"data/cirrhosis.csv\", show_col_types = FALSE)\ncirrhosis &lt;- cirrhosis %&gt;% mutate(event = if_else(Status == 'D', 1, 0)) \n\nThe main interest of this data is to explore the difference in survival time between patients taking the drug of interest, D-penicillamine, and those given a placebo. The variable Drug indicates which group the patient was in and will be used as the grouping variable for a Log-Rank Test. Below, we calculate a survival object for this data and then use the survdiff function to run a Log-Rank Test.\n\n# Calculate times and survival object\ntimes3 &lt;- Surv(cirrhosis$N_Days, cirrhosis$event)\ns3 &lt;- survfit(times3 ~ Drug, data = cirrhosis)\n# Run a log rank test based on drug group\nsurvdiff(times3 ~ Drug, data = cirrhosis)\n\nCall:\nsurvdiff(formula = times3 ~ Drug, data = cirrhosis)\n\nn=312, 106 observations deleted due to missingness.\n\n                       N Observed Expected (O-E)^2/E (O-E)^2/V\nDrug=D-penicillamine 158       65     63.2    0.0502     0.102\nDrug=Placebo         154       60     61.8    0.0513     0.102\n\n Chisq= 0.1  on 1 degrees of freedom, p= 0.7 \n\n\nFrom the Log-Rank test, we see that the p-value is 0.7, which is mush larger than 0.05. This indicates that the test was not statistically significant at the five percent level. Thus, there was no significant difference between patient outcome between the two treatment groups. For patients with biliary cirrhosis, D-penicillamine is not an effective drug for preventing death.\nWe can again visualize the difference in survival curves in Figure 2.3.\n\ns3 %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Time\",\n    y = \"Overall survival probability\",\n    title = \"Kaplan Meier Survival Curve\"\n  ) + \n  add_risktable()\n\n\n\n\nFigure 2.3: Kaplan Meier Survival Curve Predicting Death from Cirrhosis\n\n\n\n\nIt is important to note the limitations of this analysis. The Kaplan Meier Curve is not a suitable fit for a complex, real-world data set such as the cirrhosis data. It is a nonparametric method of analysis, which means that it assumes no form of distribution and no additional factors influencing the outcome of interest. To create a Kaplan Meier curve and run the Log-Rank test, we ignored all other factors included int the data. In the next section, we will discuss other methods that can be used for survival analysis that can account for other factors."
  },
  {
    "objectID": "KaplanMeier.html#references",
    "href": "KaplanMeier.html#references",
    "title": "2  Kaplan Meier",
    "section": "2.6 References",
    "text": "2.6 References\n\n\n\n\nClark, Bradburn, T. G. 2003. “Survival Analysis Part i: Basic Concepts and First Analyses.” British Journal of Cancer, May. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/.\n\n\nCollet, David. 2003. Modelling Survival Data in Medical Research. Chapman & Hall/CRC.\n\n\nFedesoriano. 2021. “Cirrhosis Prediction Dataset.” www.kaggle.com/datasets/fedesoriano/cirrhosis-prediction-dataset/data.\n\n\nGoel, Khanna, M. K. 2010. “Understanding Survival Analysis: Kaplan-Meier Estimate.” International Journal of Ayurveda Research. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059453.\n\n\nRao, & Schoenfeld, S. R. 2023. “Statistical Primer for Cardiovascular Research.” AHA Journals. https://www.ahajournals.org/doi/pdf/10.1161/circulationaha.106.614859.\n\n\nRich, Neely, J. T. 2010. “A Practical Guide to Understanding Kaplan-Meier Curves.” Otolaryngology–Head and Neck Surgery: Official Journal of American Academy of Otolaryngology-Head and Neck Surgery. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3932959/.\n\n\nSullivan, LaMorte, L. 2016. “Comparing Survival Curves.” sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival5.html.\n\n\nZabor, E. C. 2023. “Survival Analysis in r.” https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html."
  },
  {
    "objectID": "HazardAnalysis.html#introduction",
    "href": "HazardAnalysis.html#introduction",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nSo far, we have dealt with calculating survival probability using a Kaplan Meier curve. While a Kaplan Meier curve is useful for predicting survival probability for simple data, it can not always be used since its a nonparametric method, meaning it follows no prior assumptions about the data. When data becomes more complex and variables about individuals need to be considered, other methods need to be used. One of these methods, proportional hazard analysis, similarly focuses on time-to-event data, but can be used for more realistic scenarios that deal with additional explanatory variables for the event of interest (Clark (2003)). For example, we can use hazard analysis to calculate the probability of a person recovering from a disease based on type of treatment, while also including variables that could affect the outcome such as age, sex or history of drugs. A hazard function can be more useful during real-world analysis than the Kaplan Meier curve previously discussed.\nWe will again create a simple data set to demonstrate the process. Let’s recall the surv2 data set we created earlier, now with an additional variable, age Table 3.1.\n\n# Load Packages\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(survival)\nlibrary(ggsurvfit)\nlibrary(gt)\n\n\n# Recreate surv2 and add age variable\ntime &lt;- c(2, 7, 9, 8, 5, 3, 4, 10, 6, 1)\nstatus &lt;- c(0, 1, 0, 1, 0, 1, 0, 0, 1, 1)\ngroup &lt;- c(1, 1, 1, 1, 1, 2, 2 ,2, 2, 2)\nage &lt;- c(40, 62, 37, 67, 44, 70, 50, 45, 61, 62)\nsurv3 &lt;- data.frame(time, status, group, age) \n\n\nsurv3 %&gt;% gt(caption = \"Example Data Set with Status, Time, Group, and Age\") %&gt;%\n  cols_label(time = \"Time\", status = \"Status\", group = \"Group\", age = \"Age\") \n\n\n\n\n\n\n  Table 3.1:  surv table 3 \n  \n    \n    \n      Time\n      Status\n      Group\n      Age\n    \n  \n  \n    2\n0\n1\n40\n    7\n1\n1\n62\n    9\n0\n1\n37\n    8\n1\n1\n67\n    5\n0\n1\n44\n    3\n1\n2\n70\n    4\n0\n2\n50\n    10\n0\n2\n45\n    6\n1\n2\n61\n    1\n1\n2\n62"
  },
  {
    "objectID": "HazardAnalysis.html#proportional-hazards",
    "href": "HazardAnalysis.html#proportional-hazards",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.2 Proportional Hazards",
    "text": "3.2 Proportional Hazards\nThe proportional hazards model, also known as the Cox regression model, is a semi-parametric model. This is because it assumes proportional hazards, or that the hazard of an event occurring is the same for all individuals, yet does not assume a specific distribution (Collet (2003)). The proportional hazard model also assumes that the hazard ratio between two groups is constant over time. This means that the hazard ratio between two groups is the same at any time \\(t\\), thus making the model proportional. In other words, the instantaneous hazard of an event occurring between two individuals of different groups will remain constant at all times, or the effect of the predictors is the same at all times (“Cox Proportional-Hazards Model” (n.d.))."
  },
  {
    "objectID": "HazardAnalysis.html#hazard-ratios",
    "href": "HazardAnalysis.html#hazard-ratios",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.3 Hazard Ratios",
    "text": "3.3 Hazard Ratios\nThe goal of hazard analysis is to create a hazard function for modelling time to event data, where the outcome \\(h(t)\\) or \\(λ(t)\\) is the probability of the event occurring for a subject who has lasted until time \\(t\\) (Clark (2003)). The hazard function is modeled using hazard ratios, which express the ratio between the hazard of an event between two groups at a time. The hazard ratio between two groups can be expressed as \\(v = \\frac{h_1(t)}{h_2(t)}\\) (Collet (2003))."
  },
  {
    "objectID": "HazardAnalysis.html#cox-proportional-hazards-model",
    "href": "HazardAnalysis.html#cox-proportional-hazards-model",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.4 Cox Proportional Hazards Model",
    "text": "3.4 Cox Proportional Hazards Model\nThe Cox proportional hazards model is expressed in terms of the hazard function on an individual \\(i\\) at time \\(t\\). This function is expressed as \\(h_i(t) = vh_0(t)\\), where \\(h_0(t)\\) is the baseline hazard function and \\(v\\) is the hazard ratio (Collet (2003)). The baseline hazard function is the hazard function for a subject with all explanatory variables equal to zero. For the models we will discuss, the hazard ratio \\(v\\) is set to equal \\(exp(\\beta)\\) since the hazard ratio cannot be a negative value. The parameter \\(beta\\) is thus the log of the hazard ratio, expressed \\(\\beta = log(v)\\). Any value of \\(\\beta\\) will output a positive value \\(v\\). We can include many explanatory variables in the hazard function such as factors, which can take on different levels, or variates, which can be any value on a continuous scale. With many predictors, the Cox proportional hazards model can be expressed as \\(h_i(t) = h_0(t)exp(\\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_px_{pi})\\), where \\(h_0(t)\\) is the baseline hazard function, \\(x_1\\) to \\(x_p\\) are the values of the explanatory variables for individual \\(i\\), and \\(\\beta_1\\) to \\(\\beta_p\\) are the regression coefficients. The model can also be expressed as a linear model in terms of the log of the ratios between the two hazard functions, looking like \\(log(\\frac{h_i(t)}{h_0(t)}) = \\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_px_{pi}\\) (Collet (2003))."
  },
  {
    "objectID": "HazardAnalysis.html#method-of-maximum-likelihood",
    "href": "HazardAnalysis.html#method-of-maximum-likelihood",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.5 Method of Maximum Likelihood",
    "text": "3.5 Method of Maximum Likelihood\nIn the Cox proportional hazards model, there are two unknowns: the baseline hazard function, \\(h_0(t)\\), and the regression coefficients, \\(\\beta_1\\) to \\(\\beta_p\\). The method of maximum likelihood is a statistical method used to estimate values for unknown parameters by fitting a model that maximizes the likelihood of outputting the data we have (Allison (1984)). In general, the maximum likelihood estimate for an unknown value, \\(\\theta\\), is the value that maximizes the likelihood function, \\(L(\\theta) = \\prod_{i=1}^n f(x_i|\\theta)\\), where \\(f(x_i|\\theta)\\) is the probability density function, or pdf, of the data (“Maximum Likelihood Estimation” (n.d.)). The pdf and CDF, or the Cumulative Distribution Function, of the data are functions that express probability of certain values based on the distribution of the data. The baseline hazard function, \\(h_0(t)\\), can be expressed as a function of the pdf and CDF of the data, looking like \\(h_0(t) = \\frac{f(t)}{1-F(t)}\\), where \\(f(t)\\) is the pdf and \\(F(t)\\) is the CDF (Allison (1984)). However, as mentioned before, a specific distribution of the data is not assumed in Hazard Analysis. This means that the pdf, CDF, and therefore the baseline hazard function, can not be calculated using the method of maximum likelihood. Instead, the method of partial likelihood can be used to estimate the regression coefficients (Collet (2003))."
  },
  {
    "objectID": "HazardAnalysis.html#method-of-partial-likelihood",
    "href": "HazardAnalysis.html#method-of-partial-likelihood",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.6 Method of Partial Likelihood",
    "text": "3.6 Method of Partial Likelihood\nThe method of partial likelihood, first discovered by Cox himself, allows us to estimate the regression coefficients without knowing the baseline hazard function (Collet (2003)). As a reminder, the baseline hazard function describes the distribution of the data when no events have happened, but we only have data points at times when either an event occurs or an individual is censored. Thus, we need a way to fit a function that maximizes the likelihood of getting these data points without knowing what happens at every time. The method of partial likelihood does this by ranking the times of events in the data, and then using these ordered event times to predict the hazard ratio. It is called a partial likelihood method because it does not actually use the exact times of events but instead just their rankings (Collet (2003)). One key assumption of this method is that there are no ties in the data, or events at the same time . Additionally, it is important to note that this method assumes that the time intervals between each event are independent of the model parameters, or that the time intervals give no information about the model parameters (Waagepetersen (2022))."
  },
  {
    "objectID": "HazardAnalysis.html#partial-likelihood-derivation",
    "href": "HazardAnalysis.html#partial-likelihood-derivation",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.7 Partial Likelihood Derivation",
    "text": "3.7 Partial Likelihood Derivation\nThe method of partial likelihood is based on the assumption that the time intervals between events gives no information about the model parameters. Instead, the method uses the order of the events to estimate the hazard ratio. The method uses the idea of conditional probability that a certain individual has an event at time \\(t_j\\) given that they have survived until time \\(t_i\\) and that an event has occurred at time \\(t_j\\).\nThe definition of conditional probability is given by: \\(P(A|B) = \\frac{P(A ∩ B)}{P(B)}\\). This is saying that the probability of some event A given that an event B has occurred is equal to the probability that both events A and B occur divided by the probability that event B occurs. In the context of the Cox proportional hazards model, the conditional probability would look like: \\(P(\\text{individual with variables } x_i \\text{ experiences event at time } t_j | \\text{one event at time } t_j )\\) = \\(\\frac{P(\\text{individual with variables } x_i \\text{ experiences event at time } t_j)}{P(\\text{an event happens at time } t_j)}\\).\nThe probability of an event happening at \\(t_j\\) can be represented numerically as the sum of the probabilities of event for all of the individuals at risk at time \\(t_j\\), or \\({\\sum_{l \\in R(t_j)} P(\\text{individual l dies at time j}})\\). Now, we can replace these terms with the hazard functions, \\(h_i(t_j)\\) and \\(h_lt_j)\\), for the numerator and denominator, making the new probability \\(\\frac{h_i(t_j)}{\\sum_{l \\in R(t_j)} h_l(t_j)}\\) (Collet (2003)).\nRecall that the hazard ratio, \\(h_i(t)\\), is defined as the hazard function of individual i at time t, or \\(h_i(t) = h_0(t)exp(\\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_px_{pi})\\). The baseline hazard function, \\(h_0(t)\\), will thus cancel out in the conditional probability equation, simplifying it to \\(\\frac{exp(\\beta' x_j)}{\\sum_{l \\in R(t_j)} exp(\\beta' x_l)}\\), where \\(\\beta'\\) is the vector of regression coefficients and \\(x_j\\) is the vector of predictor variables for individual j (Collet (2003)).\nThe partial likelihood method states that the likelihood of the data is the product of the conditional probabilities of the events happening at the ordered event times, or \\(L(\\beta) = \\prod_{j=1}^r \\frac{exp(\\beta' x_j)}{\\sum_{l \\in R(t_j)} exp(\\beta' x_l)}\\), where there are a total of r ordered event times and j is the index of the ordered event times. \\(R(t_j)\\) is the set of individuals at risk at time \\(t_j\\), and \\(l\\) is the index of the individual within the risk set at time j. This is the likelihood of the data given the regression coefficients, \\(\\beta'\\).\nThe partial likelihood function can then be written as the natural logarithm of the likelihood function, or \\(log(L(\\beta)) = log(\\prod_{j=1}^r \\frac{exp(\\beta' x_j)}{\\sum_{l \\in R(t_j)} exp(\\beta' x_l)})\\). The goal of the method of partial likelihood is to find the values of \\(\\beta'\\) that maximize the partial likelihood function (Collet (2003)). Once we fit this equation, we can estimate these parameters by taking the derivative with respect to \\(\\beta'\\) and setting it the equation equal to zero, or \\(\\frac{\\partial l(\\beta)}{\\partial \\beta} = 0\\). This will give us the partial likelihood estimate for the regression coefficients."
  },
  {
    "objectID": "HazardAnalysis.html#example-data",
    "href": "HazardAnalysis.html#example-data",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.8 Example Data",
    "text": "3.8 Example Data\nFor the first example, lets only use one predictor variable, group. The data we will use is shown in Table 3.2.\n\nsurv4 &lt;- surv3 %&gt;% \n  mutate(id = row_number()) %&gt;%\n  select(id, time, status, group) %&gt;% \n  arrange(group, time)\n\nsurv4 %&gt;%\n  gt(caption = \"Example Data Set with ID, Time, Status, and Group\") %&gt;%\n  cols_label(id = \"ID\", time = \"Time\", status = \"Status\", group = \"Group\")\n\n\n\n\n\n\n  Table 3.2:  surv table 4 \n  \n    \n    \n      ID\n      Time\n      Status\n      Group\n    \n  \n  \n    1\n2\n0\n1\n    5\n5\n0\n1\n    2\n7\n1\n1\n    4\n8\n1\n1\n    3\n9\n0\n1\n    10\n1\n1\n2\n    6\n3\n1\n2\n    7\n4\n0\n2\n    9\n6\n1\n2\n    8\n10\n0\n2\n  \n  \n  \n\n\n\n\n\nLet’s start by calculating a hazard function between groups 1 and 2. To do this, we will assume that the baseline hazard function is constant, or \\(h_0(t) = h_0\\).\nRecall that the Cox proportional hazard analysis equation is \\(h_i(t) = h_0(t)exp(\\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_px_{pi})\\), or \\(h_i(t) = h_0(t)exp(\\beta' x_{i})\\), where \\(x_i\\) is a binary variable representing what group individual i is in. For group 1 in our example, \\(h_1(t) = h_0(t)exp(\\beta'x_{1})\\), and for group 2, \\(h_2(t) = h_0(t)exp(\\beta'x_{2})\\).\nWe can then calculate the hazard ratio between the two groups as \\(v = \\frac{h_1(t)}{h_2(t)} = \\frac{h_0(t)exp(\\beta' x_{1})}{h_0(t)exp(\\beta' x_{2})} = \\frac{exp(\\beta' x_{1})}{exp(\\beta' x_{2})}\\) (Collet (2003))."
  },
  {
    "objectID": "HazardAnalysis.html#censoring",
    "href": "HazardAnalysis.html#censoring",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.9 Censoring",
    "text": "3.9 Censoring\nSince censoring is still a concern for our data, an additional term needs to be added to the partial likelihood function, making it now: \\(L(\\beta) = \\prod_{i=1}^r [\\frac{exp(\\beta' x_i)}{\\sum_{l \\in R(t_i)} exp(\\beta' x_l)}]^{δ_i}\\), where {δ_i} is an indicator variable that is 0 if the \\(rth\\) event time is right-censored and 1 otherwise. This term is added to the likelihood function to account for the fact that the event time is not observed for right-censored individuals (Collet (2003)). When taking the log, the function becomes \\(log(L(\\beta)) = \\sum_{i=1}^r δ_i [\\beta' x_i - log \\sum_{l \\in R(t_i)} exp(\\beta' x_l)]\\) (Collet (2003))."
  },
  {
    "objectID": "HazardAnalysis.html#estimating-b",
    "href": "HazardAnalysis.html#estimating-b",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.12 Estimating B’",
    "text": "3.12 Estimating B’\nWe can then take the derivative of the log partial likelihood function with respect to \\(\\beta\\) and set it equal to 0 to find the value of \\(\\beta\\) that maximizes the partial likelihood function. The derivative will be: \\(\\frac{\\partial log(L(\\beta))}{\\partial \\beta}\\) = \\(3x_2 + 2x_1 - \\frac{5x_1exp(\\beta x_1)}{5exp(\\beta x_1) + 5exp(\\beta x_2)} - \\frac{5x_2exp(\\beta x_2)}{5exp(\\beta x_1) + 5exp(\\beta x_2)} - \\frac{4x_1exp(\\beta x_1)}{4exp(\\beta x_1) + 4exp(\\beta x_2)} - \\frac{4x_2exp(\\beta x_2)}{4exp(\\beta x_1) + 4exp(\\beta x_2)}\\) - \\(\\frac{3x_1exp(\\beta x_1)}{3exp(\\beta x_1) + 2exp(\\beta x_2)} - \\frac{2x_2exp(\\beta x_2)}{3exp(\\beta x_1) + 2exp(\\beta x_2)} - \\frac{3x_1exp(\\beta x_1)}{3exp(\\beta x_1) + exp(\\beta x_2)} - \\frac{x_2exp(\\beta x_2)}{3exp(\\beta x_1) + exp(\\beta x_2)} - \\frac{2x_1exp(\\beta x_1)}{2exp(\\beta x_1) + exp(\\beta x_2)}\\) - \\(\\frac{x_2exp(\\beta x_2)}{2exp(\\beta x_1) + exp(\\beta x_2)}\\).\nSetting this equation equal to zero and solving for \\(\\beta\\) would theoretically give us the value of \\(\\beta\\) that maximizes the partial likelihood function. However, because this equation is not solvable, we can use numerical methods to approximate the value of \\(\\beta\\). One of the most common methods is called the Newton Raphson method."
  },
  {
    "objectID": "HazardAnalysis.html#newton-raphson-method",
    "href": "HazardAnalysis.html#newton-raphson-method",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.13 Newton Raphson Method",
    "text": "3.13 Newton Raphson Method\nThe Newton Raphson method is an iterative method that is used to find the root of a function, f(x). In our case, we case use this method to find the value of \\(\\beta\\) that maximizes the partial likelihood function. We do this by starting with an initial estimate, \\({x_0}\\), for the root, and repeating a series of steps to improve this estimation. We will first chose a value for x, \\(x_0\\), and then find the equation of the tangent line to the function at the point \\((x_0, f(x_0))\\). We can then find the x-intercept of the tangent line at \\(x_0\\) by setting our function equal to 0. This x-intercept is our next estimate, \\(x_1\\), for the root (Anstee (n.d.)).\nLet f(x) be the function we want to find the root of and r be the root of the equation when f(x) = 0. Suppose \\(r = {x_0} + h\\). The value of h is the distance from the true root, r, and our initial estimate, \\(x_0\\). A key assumption of this method is that \\({x_0}\\) is a good estimate and therefore h is close to the true value. Because h is small, we can use the linear approximation of the tangent line. To do this, we know that the slope of the tangent line is the derivative of the function at the point \\((x_0, f(x_0))\\). Thus we can estimate \\(0 = f(r) = f({x_0} + h) = f({x_0}) + f'({x_0})h\\). Solving for h we get \\(h = -\\frac{f({x_0})}{f'({x_0})}\\). Thus, \\(r = {x_0} + h = {x_0} - \\frac{f({x_0})}{f'({x_0})}\\). The new value of r will be a better estimate for the root, \\(x_1\\) (Anstee (n.d.)).\nThe equation used for this iteration looks like: \\(x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\). We can repeat this step until the difference between \\({x_n+1}\\) and \\({x_n}\\) is less than a certain tolerance level, meaning that our estimate is not changing much with new iterations (Anstee (n.d.))."
  },
  {
    "objectID": "HazardAnalysis.html#multiple-predictors",
    "href": "HazardAnalysis.html#multiple-predictors",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.16 Multiple Predictors",
    "text": "3.16 Multiple Predictors\nWhen looking at multiple predictors, such as age and group, the parameter estimates are found by the equation:\nWe can calculate the cumulative hazard ratio, \\(v\\) of the event occurring in the group of individuals with a status of 1 compared to the group of individuals with a status of 0. We can calculate the hazard of the event occurring for an individual in group 1 at time t, \\(h_1(t)\\), by dividing the number of individuals who died in the group of individuals with a status of 1 by the total number of individuals in the group of individuals with a status of 1.\nThis can be expressed as \\(v = \\frac{\\frac{d_1}{n_1}}{\\frac{d_0}{n_0}}\\)."
  },
  {
    "objectID": "HazardAnalysis.html#hazard-analysis-in-r",
    "href": "HazardAnalysis.html#hazard-analysis-in-r",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.17 Hazard Analysis in R",
    "text": "3.17 Hazard Analysis in R\nSimilarly to before with the Kaplan Meier curve, we can use R to model the hazard function. The coxph() function in the survival package returns the coefficients of the cox proportional hazards model as well as the p value for the coefficients, allowing us to determine whether each coefficient is significant (“Cox Proportional-Hazards Model” (n.d.)). Below, we see that the coefficient on the age predictor is significant, but the coefficient for group is not. This means that, when controlling for the other variables, the hazard of the event occurring is not significantly different between the two groups, but the hazard of the event occurring is significantly different for individuals of different ages.\n\ncoxph(Surv(time, status) ~ group + age, data = surv3, ties = \"breslow\")\n\nCall:\ncoxph(formula = Surv(time, status) ~ group + age, data = surv3, \n    ties = \"breslow\")\n\n          coef exp(coef) se(coef)     z      p\ngroup  2.60097  13.47683  1.79663 1.448 0.1477\nage    0.18655   1.20508  0.09491 1.965 0.0494\n\nLikelihood ratio test=8.79  on 2 df, p=0.01232\nn= 10, number of events= 5"
  },
  {
    "objectID": "HazardAnalysis.html#case-study",
    "href": "HazardAnalysis.html#case-study",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.18 Case Study",
    "text": "3.18 Case Study\nLet’s look back at the Cirrhosis data set.\n\n# Load in the data and select the variables we need\ncirrhosis &lt;- read_csv(\"data/cirrhosis.csv\", show_col_types = FALSE)\ncirrhosis &lt;- cirrhosis %&gt;% \n  mutate(event = if_else(Status == 'D', 1, 0)) %&gt;% \n  select(event, Drug, N_Days)\n\nRecall that we can look at the times using the Surv() function.\n\nSurv(cirrhosis$N_Days, cirrhosis$event)\n\n  [1]  400  4500+ 1012  1925  1504+ 2503  1832+ 2466  2400    51  3762   304 \n [13] 3577+ 1217  3584  3672+  769   131  4232+ 1356  3445+  673   264  4079 \n [25] 4127+ 1444    77   549  4509+  321  3839  4523+ 3170  3933+ 2847  3611+\n [37]  223  3244  2297  4467+ 1350  4453+ 4556+ 3428  4025+ 2256  2576+ 4427+\n [49]  708  2598  3853  2386  1000  1434  1360  1847  3282  4459+ 2224  4365+\n [61] 4256+ 3090   859  1487  3992+ 4191  2769  4039+ 1170  3458+ 4196+ 4184+\n [73] 4190+ 1827  1191    71   326  1690  3707+  890  2540  3574  4050+ 4032+\n [85] 3358  1657   198  2452+ 1741  2689   460   388  3913+  750   130  3850+\n [97]  611  3823+ 3820+  552  3581+ 3099+  110  3086  3092+ 3222  3388+ 2583 \n[109] 2504+ 2105  2350+ 3445   980  3395  3422+ 3336+ 1083  2288   515  2033+\n[121]  191  3297+  971  3069+ 2468+  824  3255+ 1037  3239+ 1413   850  2944+\n[133] 2796  3149+ 3150+ 3098+ 2990+ 1297  2106+ 3059+ 3050+ 2419   786   943 \n[145] 2976+ 2615+ 2995+ 1427   762  2891+ 2870+ 1152  2863+  140  2666+  853 \n[157] 2835+ 2475+ 1536  2772+ 2797+  186  2055   264  1077  2721+ 1682  2713+\n[169] 1212  2692+ 2574+ 2301+ 2657+ 2644+ 2624+ 1492  2609+ 2580+ 2573+ 2563+\n[181] 2556+ 2555+ 2241+  974  2527+ 1576   733  2332+ 2456+ 2504+  216  2443+\n[193]  797  2449+ 2330+ 2363+ 2365+ 2357+ 1592+ 2318+ 2294+ 2272+ 2221+ 2090 \n[205] 2081  2255+ 2171+  904  2216+ 2224+ 2195+ 2176+ 2178+ 1786  1080  2168+\n[217]  790  2170+ 2157+ 1235  2050+  597   334  1945+ 2022+ 1978+  999  1967+\n[229]  348  1979+ 1165  1951+ 1932+ 1776+ 1882+ 1908+ 1882+ 1874+  694  1831+\n[241]  837+ 1810+  930  1690  1790+ 1435+  732+ 1785+ 1783+ 1769+ 1457+ 1770+\n[253] 1765+  737+ 1735+ 1701+ 1614+ 1702+ 1615+ 1656+ 1677+ 1666+ 1301+ 1542+\n[265] 1084+ 1614+  179  1191  1363+ 1568+ 1569+ 1525+ 1558+ 1447+ 1349+ 1481+\n[277] 1434+ 1420+ 1433+ 1412+   41  1455+ 1030+ 1418+ 1401+ 1408+ 1234+ 1067+\n[289]  799  1363+  901+ 1329+ 1320+ 1302+  877+ 1321+  533+ 1300+ 1293+  207 \n[301] 1295+ 1271+ 1250+ 1230+ 1216+ 1216+ 1149+ 1153+  994+  939+  839+  788+\n[313] 4062+ 3561  2844+ 2071  3030+ 1680+   41  2403+ 1170+ 2011  3523+ 3468+\n[325] 4795+ 1236+ 4214+ 2111  1462  1746    94   785  1518   466  3527+ 2635+\n[337] 2286   791  3492+ 3495+  111  3231+  625  3157+ 3021+  559  2812  2834+\n[349] 2855+  662   727  2716+ 2698+  990  2338+ 1616  2563+ 2537+ 2534+  778 \n[361]  617+ 2267+ 2249+  359  1925+  249  2202+   43  1197  1095   489  2149+\n[373] 2103+ 1980+ 1347+ 1478  1987+ 1168   597  1725+ 1899+  221  1022+ 1639+\n[385] 1635+ 1654+ 1653+ 1560+ 1581+ 1419+ 1443+ 1368+  193  1367+ 1329+ 1343+\n[397] 1328+ 1375+ 1260+ 1223+  935   943+ 1141+ 1092+ 1150+  703  1129+ 1086+\n[409] 1067+ 1072+ 1119+ 1097+  989+  681  1103+ 1055+  691+  976+\n\n\nLet’s see what the Cox Analysis will tell us about the relationship between the drug and the number of days until death.\n\ns4 &lt;- coxph(Surv(N_Days, event) ~ Drug, data = cirrhosis, ties = \"breslow\")\nsummary(s4)\n\nCall:\ncoxph(formula = Surv(N_Days, event) ~ Drug, data = cirrhosis, \n    ties = \"breslow\")\n\n  n= 312, number of events= 125 \n   (106 observations deleted due to missingness)\n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)\nDrugPlacebo -0.05712   0.94448  0.17917 -0.319     0.75\n\n            exp(coef) exp(-coef) lower .95 upper .95\nDrugPlacebo    0.9445      1.059    0.6648     1.342\n\nConcordance= 0.499  (se = 0.025 )\nLikelihood ratio test= 0.1  on 1 df,   p=0.7\nWald test            = 0.1  on 1 df,   p=0.7\nScore (logrank) test = 0.1  on 1 df,   p=0.7\n\n\nThe p value for the drug is not less than 0.05, so we can not conclude that the drug has a significant effect on the number of days until death."
  },
  {
    "objectID": "HazardAnalysis.html#references",
    "href": "HazardAnalysis.html#references",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.19 References",
    "text": "3.19 References\n\n\n\n\nAllison, P. 1984. Event History Analysis. SAGE Publications, Inc. https://methods.sagepub.com/book/event-history-analysis/.\n\n\nAnstee, R. n.d. The Newton-Raphson Method. https://personal.math.ubc.ca/~anstee/math104/newtonmethod.pdf.\n\n\nClark, Bradburn, T. G. 2003. “Survival Analysis Part i: Basic Concepts and First Analyses.” British Journal of Cancer, May. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/.\n\n\nCollet, David. 2003. Modelling Survival Data in Medical Research. Chapman & Hall/CRC.\n\n\n“Cox Proportional-Hazards Model.” n.d. www.sthda.com/english/wiki/cox-proportional-hazards-model.\n\n\n“Maximum Likelihood Estimation.” n.d. https://online.stat.psu.edu/stat415/lesson/1/1.2.\n\n\nWaagepetersen, Rasmus. 2022. “Cox’s Proportional Hazards Model and Cox’s Partial Likelihood.” https://people.math.aau.dk/~rw/Undervisning/DurationAnalysis/Slides/lektion3.pdf."
  },
  {
    "objectID": "HazardAnalysis.html#ties-in-the-data",
    "href": "HazardAnalysis.html#ties-in-the-data",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.10 Ties in the Data",
    "text": "3.10 Ties in the Data\nOne of the key assumptions we are making is that there are no ties in event or censor times. However, this is not always the case. There are additional models proposed to account for ties in the data, such as the Efron method and the Breslow method, which add weights to the likelihood function to account for ties in the data. Later, we will use the Breslow method in analysis because it is the simplest. Breslow suggested the equation: \\(L(B) = \\prod_{i=1}^{r} \\frac{exp(\\beta' s_i)}{(\\sum_{l \\in R(t_i)} exp(\\beta' x_l))^{d_i}}\\), where \\(d_i\\) is the number of individuals at risk at time \\(t_i\\) (Collet (2003)). For now, we will assume there are no ties in the data and use the partial likelihood function without weights."
  },
  {
    "objectID": "HazardAnalysis.html#fitting-the-partial-log-likelihood-equation",
    "href": "HazardAnalysis.html#fitting-the-partial-log-likelihood-equation",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.11 Fitting the Partial Log Likelihood Equation",
    "text": "3.11 Fitting the Partial Log Likelihood Equation\nTo fit our likelihood function, we must order all of the event times. For group 1, there are only two times of event, time t = 7 for individual 2 and time t = 8 for individual 4. So, we will order the event times as \\(t_{(1)} &lt; t_{(2)}\\), where \\(t_{(1)}\\) represents the event time for individual 2 and \\(t_{(2)}\\) represents the event time for individual 4. Similarly for group 2, there are events at times 1, 3, and 6, so we will order the event times as \\(t_{(1)} &lt; t_{(2)} &lt; t_{(3)}\\), representing the event times for individuals 10, 6, and 9. We will also need to calculate the risk set at each time, which is shown below in Table 3.3.\n\nrisk &lt;- c(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n\nsurv5 &lt;- surv4 %&gt;% \n  arrange(time) %&gt;% \n  mutate(risk_set = risk) %&gt;%\n  select(id, group, time, status, risk_set) \n\nsurv5 %&gt;%\n  gt(caption = \"Example Data Set with Status, Group, Time, and Risk Set\") %&gt;%\n  cols_label(id = \"ID\", group = \"Group\", time = \"Time\", status = \"Status\", risk_set = \"Risk Set\")\n\n\n\n\n\n\n  Table 3.3:  surv table 5 \n  \n    \n    \n      ID\n      Group\n      Time\n      Status\n      Risk Set\n    \n  \n  \n    10\n2\n1\n1\n10\n    1\n1\n2\n0\n9\n    6\n2\n3\n1\n8\n    7\n2\n4\n0\n7\n    5\n1\n5\n0\n6\n    9\n2\n6\n1\n5\n    2\n1\n7\n1\n4\n    4\n1\n8\n1\n3\n    3\n1\n9\n0\n2\n    8\n2\n10\n0\n1\n  \n  \n  \n\n\n\n\n\nTo estimate the parameters, we will need to use the natural log of our partial likelihood function, which we derived as: \\(log(L(\\beta)) = \\sum_{i=1}^r δ_i [\\beta' x_i - log \\sum_{l \\in R(t_i)} exp(\\beta' x_l)]\\).\nAt time \\(t=1\\), the risk set will include all 10 individuals because at the tie just before t=1, there are no individuals who have experienced the event or been censored. At time \\(t=2\\), the risk set will include all individuals except for individuals with ID 10 because they experienced the event at time 1. We can continue this process for each time, and the risk set will be the same for both groups.\nNow that we know the risk sets and the event times, we can calculate the partial likelihood function. Recall that the partial likelihood function is \\(L(\\beta) = \\prod_{i=1}^{n} \\frac{exp(\\beta x_i)}{\\sum_{j \\in R(t_i)} exp(\\beta x_j)}\\). So, for our data set, we will need to calculate the partial likelihood function for each event time. For each relevant time, we can calculate the risk score for individual i, \\(v(i) = B'{x_i}\\) (Collet (2003)). This risk score represents the numerator of the partial likelihood equation. For time \\(t = 1\\), in which individual 10 experiences the event of interest, the risk score is denoted \\(v(10)\\). The denominator of the partial likelihood equation is the sum of the risk scores for all individuals in the risk set at time \\(t_i\\), so all individuals 1 through 10. The denominator is then \\(v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)\\). The partial likelihood function for time \\(t = 1\\) is then \\(\\frac{v(10)}{v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)}\\).\nAfter doing this for each event time, we get the partial likelihood equation to be \\(\\frac{v(10)}{v(1) + v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9) + v(10)}\\) * \\(\\frac{v(6)}{v(2) + v(3) + v(4) + v(5) + v(6) + v(7) + v(8) + v(9)}\\) * \\(\\frac{v(9)}{v(2) + v(3) + v(4) + v(8) + v(9)}\\) * \\(\\frac{v(2)}{v(2) + v(3) + v(4) + v(8)}\\) * \\(\\frac{v(4)}{v(3) + v(4) + v(8)}\\).\nWe can then take the log of the partial likelihood function to get the log partial likelihood function: \\(log(L(B)) = 3\\beta x_2 + 2\\beta x_1 - log(5exp(\\beta x_1) + 5exp(\\beta x_2))\\) - \\(log(4exp(\\beta x_1) + 4exp(\\beta x_2)) - log(3exp(\\beta x_1) + 2exp(\\beta x_2)) - log(3exp(\\beta x_1)\\) + \\(exp(\\beta x_2)) - log(2exp(\\beta x_1) + exp(\\beta x_2))\\)."
  },
  {
    "objectID": "HazardAnalysis.html#newton-raphson-example",
    "href": "HazardAnalysis.html#newton-raphson-example",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.14 Newton Raphson Example",
    "text": "3.14 Newton Raphson Example\nLet’s consider a simple example of a Newton Raphson iteration. Suppose we want to find the root of the function $f(x) =\n\ngamma distribution: \\(f(x) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}\\)."
  },
  {
    "objectID": "HazardAnalysis.html#newton-raphson-in-cox-proportional-hazards-model",
    "href": "HazardAnalysis.html#newton-raphson-in-cox-proportional-hazards-model",
    "title": "3  Cox Proportional Hazards Regression",
    "section": "3.15 Newton Raphson in Cox Proportional Hazards Model",
    "text": "3.15 Newton Raphson in Cox Proportional Hazards Model\nIn the Cox Proportional Hazards Model, the Newton Raphson method is used to find the value of \\(\\beta\\) that maximizes the partial likelihood function. Let’s assume that there is only one parameter, \\(\\beta\\), in the model. The partial likelihood function is given by:"
  }
]